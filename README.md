# Back End Exercise
Coding exercise for evaluation purposes.

This project is a RESTful Web Service implemented with Flask, Python, and uses MongoDB as database. It supports a front end (not included here) to process, save, and retrieve logs on demand.

## Technologies and dependencies
Python 3.6.6 and MongoDB Community Server 4.4.1 were used in the development of this project. It uses several Python modules/packages/libraries - make sure to install them, if required:
- flask
- dateutil
- pymongo
- requests

## How-to
If all requirements are met, the application can be started by defining the FLASK_APP environment variable and using the flask command. For example, in Windows, using PowerShell:
```
$env:FLASK_APP = "main.py"
python -m flask run
```
The folder **Test** contains files to test the functionality of the API (*test_api.py*) and to test its performance under multiple concurrent requests (*performance.py*). Both require the Flask application to be running.
- To run the functionality test, simply use **pytest** in the command line interpreter.
- To run the performance test, run *performance.py* file.

## General comments and discussion

1. Implementation decisions
- Although the provided sample log was composed by multiple individual logs to improve performance with a more compact data representation, I decided to keep them separated. However, this structure could be easily generated by grouping the logs based on the user and session ids.
- I introduced an overhead in the datetime processing since the comparison of strings of dates is not encouraged. As a result, the dates and times are being stored in the database in UTC. It can easily be converted to any timezone on demand.
- Due to the apparent dynamic structure of the logs and the massive amount of data expected to be handled, I opted for a NoSQL database. A structured database would saturate quickly under the expected level of interaction.
- I chose to use PyMongo instead of MongoEngine to interact with the database. Although MongoEngine would be a great choice for the creation of schemas through classes to manipulate data in a more organized manner, there's some discussion regarding its performance when dealing with larger documents and/or collections. Therefore, to focus on performance, I used PyMongo to have finer (lower level) control of operations.

2. Testing
- I have included a basic testing scenario for the API, mainly checking if the operations were consistent with what is requested.
- The performance test tries to simulate multiple concurrent requests sent to the application. The requirements of the exercise specify that the front end will send logs every 5 minutes from up to 100 users, but there is no information regarding the amount of data being transmitted. I set up a basic test to simulate **100 users** sending **5 logs** per request for a total of **50 times** sequentially, which satisfies the requirements. The obtained results (presented below) show that the current application is able to process an average of 403 requests per second.
```
-----------------Test Statistics-----------------
Total execution time (s): 12.38
Concurrent users: 100
Logs per user: 5
Requests per user: 50

Total tests: 5000 | Pass: 5000 | Fail: 0
Requests per second: 403.9
Request duration (s) - average: 0.232 | min: 0.018 | max: 0.390
```

3. Scalability
- In its current state, the application is **not** suitable for production due to poor scalability. I intended to deploy it on Google Cloud but I encountered several problems which I couldn't solve in the time being.
- The preliminar tests show that the current basic application is able to handle a decent amount of requests. However, to get to the point in which thousands of users could interact with the application simultaneously and a massive amount of data could be collected, some strategies are required.
  - The data representation can be optimized depending on the use case, just like what happens in the provided sample log (compact representation).
  - Optimizations in the database can help increasing the performance of queries. For instance, indices were created for the fields that I saw as essential, but not knowing the most frequently accessed data, they could even hurt the performance.
  - When the focus is to simply handle more users/requests, the ability to scale the application reaches a limit quite fast. However, setting up new instances of the application is a more direct approach - or even creating containers and deploying to several servers (not something I have experience with, unfortunately). The idea of distributing processing and using a load balancer can be helpful.

## REST API Endpoints
The following documentation considers that the application is running locally, so the URL is defined by the local ip address (127.0.0.1) and default port used by Flask (5000).

### */save_log* ###
#### Description ####
API call to insert a single log into the database.
#### URL Structure ####
http://127.0.0.1:5000/save_log
#### Method ####
POST
#### Data ####
A log to be inserted in JSON format.
```
{
 "userId":"ABC123",
 "sessionId":"DEF456",
 "type":"XYZ",
 "time":"2020-09-12T22:37:28-06:00",
 "properties":{}
}
```
#### Returns ####
Message containing the internal id of the log in the database.
```
"Log saved successfully with id 5f5b2c71d542c0692f4cf792."
```

### */save_logs* ###
#### Description ####
API call to insert multiple logs into the database.
#### URL Structure ####
http://127.0.0.1:5000/save_logs
#### Method ####
POST
#### Data ####
A list of logs to be inserted in JSON format.
```
[{
 "userId":"ABC123",
 "sessionId":"DEF456",
 "type":"XYZ",
 "time":"2020-09-12T22:37:28-06:00",
 "properties":{}
},
{
 "userId":"MNO789",
 "sessionId":"QRS123",
 "type":"UIT",
 "time":"2020-09-10T12:20:00-06:00",
 "properties":{}
}]
```
#### Returns ####
Message indicating the number of logs added to the database.
```
"2 logs saved successfully."
```


### */get_log* ###
#### Description ####
API call to retrieve logs that satisfy a given set of conditions.
#### URL Structure ####
http://127.0.0.1:5000/get_logs
#### Method ####
GET
#### Data ####
A filter specifying the conditions to search for logs. The conditions may include any combination of: user id, type, and/or time range (represented by *start* and *end*). If not provided, all logs are returned.
```
{
 "userId":"ABC123",
 "type":"XYZ",
 "start":"2020-08-01T10:00:00-06:00",
 "end":"2020-09-20T10:00:00-06:00"
}
```
#### Returns ####
The logs that satisfy the given conditions.
```
[{
 "userId":"ABC123",
 "sessionId":"DEF456",
 "type":"XYZ",
 "time":"2020-09-12T22:37:28-06:00",
 "properties":{}
}]
```

### Errors ###
Several errors can be captured, mainly due to bad formatting or lack of required data in the requests. The errors captured in the endpoints include:
- *"Empty body content in request."* (HTTP 400)
- *"Invalid data in request."* (HTTP 400)
- *"Invalid body content in request."* (HTTP 400)
- *"Invalid datetime format."* (HTTP 400)
- *"Error processing request."* (HTTP 500)
- *"Error saving log(s)."* (HTTP 500)
